{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN on CIFAR10 dataset with FishLeg.\n",
    "\n",
    "This notebook demonstrates how to train a small CNN with FishLeg, comparing it to the baseline Adam optimiser. It also provides insights into the implementation similarities of training with FishLeg and other optimisers. Additionally, FishLeg outperforms Adam in terms of runtime and epochs. The paper can be accessed [here](https://openreview.net/pdf?id=c9lAOPvQHS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Install and import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt \n",
    "!pip install -q pandas\n",
    "!pip install -q torchsummary\n",
    "!pip install -q torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from utils import class_accuracy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from data_utils import read_data_sets, get_MNIST, read_cifar\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from optim.FishLeg import FishLeg, FISH_LIKELIHOODS, initialise_FishModel\n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "import copy\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up GPU environment for model training\n",
    "\n",
    "This is to ensure that if a GPU is available it will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "seed = 13\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Running on\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Construct the function for training model\n",
    "\n",
    "The following function is written to take in different models and train them on the given dataset.\n",
    "The training is done using the choice of optimiser with the `opt` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, opt, likelihood, class_accuracy, epochs=100, device='cuda', savedir=False):\n",
    "    '''\n",
    "    Function to train model and obtain metrics per step and per epoch\n",
    "\n",
    "    Inputs:\n",
    "        model: model to train\n",
    "        train_loader: training data loader\n",
    "        test_loader: test data loader\n",
    "        opt: optimiser\n",
    "        likelihood: likelihood function\n",
    "        epochs: number of epochs to train for\n",
    "        device: device to train on\n",
    "\n",
    "    Outputs:\n",
    "        model: trained model\n",
    "        train_df_per_step: dataframe of training loss, accuracy and time per step\n",
    "        test_df_per_step: dataframe of test loss, accuracy and time per step\n",
    "        df_per_epoch: dataframe of training and test loss, accuracy and time per epoch \n",
    "    '''\n",
    "    train_df_per_step = pd.DataFrame(columns=['loss', 'acc', 'step_time', 'aux_loss'])\n",
    "    test_df_per_step = pd.DataFrame(columns=['loss', 'acc'])\n",
    "    df_per_epoch = pd.DataFrame(columns=['train_loss', 'train_acc', 'epoch_time', 'test_loss', 'test_acc'])\n",
    "    st = time.time()\n",
    "    eval_time = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            running_loss = 0\n",
    "            running_acc = 0\n",
    "            running_aux_loss = 0\n",
    "            for n, (batch_data, batch_labels) in enumerate(tepoch, start=1):\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                output = model(batch_data)\n",
    "\n",
    "                loss = likelihood(output, batch_labels)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_acc += class_accuracy(output, batch_labels).item()\n",
    "\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                et = time.time()     \n",
    "                try:\n",
    "                    aux_loss = opt.aux_loss\n",
    "                    if aux_loss != np.nan:\n",
    "                        running_aux_loss += opt.aux_loss\n",
    "                    df_temp = pd.DataFrame([[loss.item(), class_accuracy(output, batch_labels).item(), et-st, aux_loss]], columns=['loss', 'acc', 'step_time', 'aux_loss'])\n",
    "\n",
    "                except:\n",
    "                    df_temp = pd.DataFrame([[loss.item(), class_accuracy(output, batch_labels).item(), et-st]], columns=['loss', 'acc', 'step_time'])\n",
    "\n",
    "                if train_df_per_step.empty:\n",
    "                    train_df_per_step = df_temp\n",
    "                else:\n",
    "                    train_df_per_step = pd.concat([train_df_per_step, df_temp], ignore_index=True)\n",
    "\n",
    "                if n % 50 == 0:\n",
    "                    model.eval()\n",
    "\n",
    "                    running_test_loss = 0\n",
    "                    running_test_acc = 0\n",
    "\n",
    "                    for m, (test_batch_data, test_batch_labels) in enumerate(test_loader, start=1):\n",
    "                        test_batch_data, test_batch_labels = test_batch_data.to(device), test_batch_labels.to(device)\n",
    "\n",
    "                        test_output = model(test_batch_data)\n",
    "\n",
    "                        test_loss = likelihood(test_output, test_batch_labels)\n",
    "\n",
    "                        running_test_loss += test_loss.item()\n",
    "                        running_test_acc += class_accuracy(test_output, test_batch_labels).item()\n",
    "\n",
    "                        df_temp = pd.DataFrame([[test_loss.item(), class_accuracy(test_output, test_batch_labels).item()]], columns=['loss', 'acc'])\n",
    "                        if test_df_per_step.empty:\n",
    "                            test_df_per_step = df_temp\n",
    "                        else:\n",
    "                            test_df_per_step = pd.concat([test_df_per_step, df_temp], ignore_index=True)\n",
    "\n",
    "                    running_test_loss /= m\n",
    "                    running_test_acc /= m\n",
    "\n",
    "                    tepoch.set_postfix(acc=100 * running_acc / n, test_acc=running_test_acc * 100)\n",
    "                    model.train()\n",
    "                    eval_time += time.time() - et\n",
    "            \n",
    "            epoch_time = time.time() - st - eval_time\n",
    "            tepoch.set_postfix(loss=running_loss / n, test_loss=running_test_loss, epoch_time=epoch_time)\n",
    "\n",
    "\n",
    "            df_temp = pd.DataFrame([[running_loss / n, 100 * running_acc / n, epoch_time, running_test_loss, 100 * running_test_acc, running_aux_loss/n]], columns=['train_loss', 'train_acc', 'epoch_time', 'test_loss', 'test_acc', 'aux_loss'])\n",
    "\n",
    "            if df_per_epoch.empty:\n",
    "                df_per_epoch = df_temp\n",
    "            else:\n",
    "                df_per_epoch = pd.concat([df_per_epoch, df_temp], ignore_index=True)\n",
    "\n",
    "            if savedir:\n",
    "                if not os.path.exists(savedir):\n",
    "                    os.makedirs(savedir)\n",
    "                    os.makedirs(f\"{savedir}/ckpts\")\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimiser_state_dict': opt.state_dict(),\n",
    "                    'metrics': df_per_epoch,\n",
    "                    }, f\"{savedir}/ckpts/epoch={epoch}-test_loss={round(running_test_loss, 4)}.pt\")\n",
    "\n",
    "    return model, train_df_per_step, test_df_per_step, df_per_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Reading in CIFAR data\n",
    "\n",
    "First the data has to be loaded using the `read_cifar` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_cifar(\"../data/\", if_autoencoder=False)\n",
    "\n",
    "## Dataset\n",
    "train_dataset = dataset.train\n",
    "test_dataset = dataset.test\n",
    "print()\n",
    "# Use len() to get the number of examples\n",
    "print(\"Number of training samples: \", len(train_dataset))\n",
    "print(\"Number of testing samples: \", len(test_dataset))\n",
    "\n",
    "# Accessing the shape of the images\n",
    "# Assuming your dataset returns a tuple of (image, label), you can get the shape of the first image as an example\n",
    "# This line may need to be adjusted depending on how your dataset is structured\n",
    "print(\"Image shape: \", train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(dataset, n_images):\n",
    "    '''\n",
    "    Function to plot images from CIFAR dataset\n",
    "    Inputs:\n",
    "        dataset: CIFAR dataset\n",
    "        n_images: number of images to plot\n",
    "    '''\n",
    "    fig, axs = plt.subplots(1, n_images, figsize=(n_images * 2, 2))\n",
    "    if n_images == 1:  \n",
    "        axs = [axs]\n",
    "    for i, ax in enumerate(axs):\n",
    "        img = dataset[i][0]\n",
    "        label = dataset[i][1]\n",
    "        if img.shape[0] == 3: \n",
    "            img = img.permute(1, 2, 0)  \n",
    "\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot example CIFAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(train_dataset, 5)\n",
    "plot_images(test_dataset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up training and testing dataloader.\n",
    "\n",
    "Note that an additional aux_loader is defined. This is used to calculate the auxiliary loss when using FishLeg optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "aux_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, shuffle=True, batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialise the model\n",
    "\n",
    "The following code constructs a vanilla CNN model.\n",
    "\n",
    "To use FishLeg optimiser, the model has to be slightly modified so that it contains additional parameters necessary.\n",
    "<br>\n",
    "This could be done by just passing through the initialised model through the helper funcion `initialise_FishModel`.\n",
    "<br>\n",
    "This modified model can be shown by using `summary` function, both models have the same architecture but the layer name for the FishLeg model is renamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        in_channels=3,\n",
    "        out_channels=16,\n",
    "        kernel_size=5,\n",
    "        stride=1,\n",
    "        padding=2,\n",
    "    ),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Conv2d(\n",
    "        in_channels=16,\n",
    "        out_channels=32,\n",
    "        kernel_size=5,\n",
    "        stride=1,\n",
    "        padding=2,\n",
    "    ),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(8 * 16 * 16, 10),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "adam_model = copy.deepcopy(model)\n",
    "\n",
    "scale_factor = 1\n",
    "damping = 0.1\n",
    "fishleg_model = initialise_FishModel(\n",
    "    copy.deepcopy(model), module_names=\"__ALL__\", fish_scale=scale_factor / damping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam CNN summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(adam_model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FishLeg CNN summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(fishleg_model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training with Adam\n",
    "\n",
    "The work flow:\n",
    "\n",
    "- We specify a custom implementation of softmax likelihood which defines the way to compute loss in our classification tasks.\n",
    "- We then specify the hyperparameters for training: Learning rate, weight decay and optimisers.\n",
    "- Lastly, we train the model with these hyperparameters and the data specified above.\n",
    "\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "- Learning rate: Controls the step size in updating weights during training.\n",
    "- Weight decay: Adds a penalty on large weights to reduce overfitting and improve model generalization.\n",
    "- optimiser: An algorithm that adjusts weights adaptively for each parameter to minimize the loss function more efficiently.\n",
    "\n",
    "Outputs:\n",
    "\n",
    "- Trained model.\n",
    "- 3 pandas dataframe:\n",
    "    - train_df_per_step: contains metrics on training data per step (ie training loss, training accuracy, time taken at each step).\n",
    "    - test_df_per_step: contains metrics on testing data per step (ie testing loss, testing accuracy).\n",
    "    - df_per_epoch: contains metrics on training and testing data per epoch ((ie training/testing loss, training/tesing accuracy, time taken at each epoch)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = FISH_LIKELIHOODS[\"softmax\"](device=device)\n",
    "\n",
    "lr = 0.0005\n",
    "weight_decay = 1e-5\n",
    "epoch = 100\n",
    "\n",
    "opt = optim.Adam(\n",
    "    adam_model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "# savedir=None\n",
    "savedir = f\"runs/CIFAR_adam/lr={lr}_lambda={weight_decay}/{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "adam_trained_model, adam_train_df_per_step, adam_test_df_per_step, adam_df_per_epoch = train_model(adam_model, train_loader, test_loader, opt, likelihood,class_accuracy, epochs=epoch, device=device, savedir=savedir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Training with FishLeg\n",
    "\n",
    "The workflow:\n",
    "\n",
    "- Similar to Adam optimiser, we specify a custom implementation of softmax likelihood which defines the way to compute loss in our classification tasks.\n",
    "- We then specify all the hyperparameters for training: Learning rate, weight decay and optimisers.\n",
    "- Note that with FishLeg, we have additional hyperparameters responsible for the auxiliary loss training (aux_lr, aux_eps, etc.)\n",
    "- Lastly, we train the model with these hyperparameters and the data specified above.\n",
    "- Reminder, the model has to be initialised in a special way to allow training with FishLeg optimiser. This is done in Step 5.\n",
    "\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "- Learning rate: Controls the step size in updating weights during training.\n",
    "- Weight decay: Adds a penalty on large weights to reduce overfitting and improve model generalization.\n",
    "- optimiser: An algorithm that adjusts weights adaptively for each parameter to minimize the loss function more efficiently.\n",
    "- beta: coefficient for running averages of gradient (default: 0.9).\n",
    "- aux_lr: learning rate for the auxiliary parameters, using Adam (default: 1e-3).\n",
    "- aux_eps: Term added to the denominator to improve numerical stability for auxiliary parameters (default: 1e-8).\n",
    "- damping: Static damping applied to Fisher matrix, :math:\\gamma,for stability when FIM becomes near-singular (default: 5e-1).\n",
    "\n",
    "Outputs are similar to adam but with an additional auxiliary loss in the train_df_per_step dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.02\n",
    "beta = 0.9\n",
    "weight_decay = 1e-5\n",
    "likelihood = FISH_LIKELIHOODS[\"softmax\"](device=device)\n",
    "aux_lr = 1e-4\n",
    "aux_eps = 1e-8\n",
    "scale_factor = 1\n",
    "damping = 0.1\n",
    "update_aux_every = 3\n",
    "\n",
    "initialization = \"normal\"\n",
    "normalization = True\n",
    "\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(opt.state_dict()['state']))  # This should match the number of model parameters\n",
    "print(len(list(fishleg_model.parameters())))  # This prints the number of model parameters\n",
    "\n",
    "# Verify each parameter group if using them\n",
    "for group in opt.param_groups:\n",
    "    for p in group['params']:\n",
    "        if p.requires_grad:\n",
    "            print(p.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1 = FishLeg(\n",
    "    fishleg_model,\n",
    "    aux_loader,\n",
    "    likelihood,\n",
    "    lr=lr,\n",
    "    beta=beta,\n",
    "    weight_decay=weight_decay,\n",
    "    aux_lr=aux_lr,\n",
    "    aux_betas=(0.9, 0.999),\n",
    "    aux_eps=aux_eps,\n",
    "    damping=damping,\n",
    "    update_aux_every=update_aux_every,\n",
    "    method=\"antithetic\",\n",
    "    method_kwargs={\"eps\": 1e-4},\n",
    "    precondition_aux=True,\n",
    "    aux_log = True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savedir=None\n",
    "savedir = f\"./runs/CIFAR_fishleg/lr={lr}_lambda={weight_decay}/{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "fishleg_trained_model, fishleg_train_df_per_step, fishleg_test_df_per_step, fishleg_df_per_epoch = train_model(fishleg_model, train_loader, test_loader, opt1, likelihood,class_accuracy, epochs=epoch, device=device, savedir=savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualise the performance\n",
    "The performance is being visualised and Adams serves as a basline comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(adam_train_df_per_step['loss'], label=\"Adam\",color = 'blue')\n",
    "plt.plot(fishleg_train_df_per_step['loss'], label=\"FishLeg\", color = 'orange')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(adam_train_df_per_step['acc'], label=\"Adam\",color = 'blue')\n",
    "plt.plot(fishleg_train_df_per_step['acc'], label=\"FishLeg\", color = 'orange')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Training Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(adam_train_df_per_step['step_time'], adam_train_df_per_step['loss'], label=\"Adam\", color = 'blue')\n",
    "plt.plot(fishleg_train_df_per_step['step_time'], fishleg_train_df_per_step['loss'], label=\"FishLeg\", color = 'orange')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(adam_train_df_per_step['step_time'], adam_train_df_per_step['acc'], label=\"Adam\", color = 'blue')\n",
    "plt.plot(fishleg_train_df_per_step['step_time'], fishleg_train_df_per_step['acc'], label=\"FishLeg\", color = 'orange')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Training Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(adam_df_per_epoch['train_loss'], 'g-', label=\"Adam train\", color = 'blue')\n",
    "plt.plot(adam_df_per_epoch['test_loss'],'g--',label=\"Adam test\", color = 'blue')\n",
    "plt.plot(fishleg_df_per_epoch['train_loss'], 'r-', label=\"FishLeg train\", color = 'orange')\n",
    "plt.plot(fishleg_df_per_epoch['test_loss'], 'r--',label=\"FishLeg test\", color = 'orange')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(adam_df_per_epoch['train_acc'], 'g-', label=\"Adam train\", color = 'blue')\n",
    "plt.plot(adam_df_per_epoch['test_acc'], 'g--', label=\"Adam test\", color = 'blue')\n",
    "plt.plot(fishleg_df_per_epoch['train_acc'], 'r-', label=\"FishLeg train\", color = 'orange')\n",
    "plt.plot(fishleg_df_per_epoch['test_acc'], 'r--', label=\"FishLeg test\", color = 'orange')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(adam_df_per_epoch['epoch_time'], adam_df_per_epoch['train_loss'], label=\"Adam\", color = 'blue')\n",
    "plt.plot(fishleg_df_per_epoch['epoch_time'], fishleg_df_per_epoch['train_loss'], label=\"FishLeg\", color = 'orange')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Training loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(adam_df_per_epoch['epoch_time'], adam_df_per_epoch['train_acc'], label=\"Adam\", color = 'blue')\n",
    "plt.plot(fishleg_df_per_epoch['epoch_time'], fishleg_df_per_epoch['train_acc'], label=\"FishLeg\", color = 'orange')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Training accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "As displayed, with FishLeg optimiser, it took longer to run the same number of epoches but we are able to converge quicker in comparison to Adam optimiser in terms of both steps and time, with also higher accuracy overall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Loss\n",
    "\n",
    "To better understand the operation of FishLeg, we could also plot the auxiliary loss throughout the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fishleg_train_df_per_step['aux_loss'], label=\"FishLeg Auxiliary Loss\", color = 'green')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Auxiliary loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fishleg_df_per_epoch['aux_loss'], label=\"FishLeg Auxiliary Loss\", color = 'green')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Auxiliary loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "dee4cb6648085172192ebedd48c9a6f014ffcad5925711ae86db60fc7a3e5218"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
